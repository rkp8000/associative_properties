{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title\n",
    "\n",
    "Efficient feature-binding in short-term memory in a random neural network without Hebbian plasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Short-term memory is thought to play a key role in high-level cognition, but relative to that of other neural systems its mechanistic basis is poorly understood. While at least some degree of short-term memory exists in any off-the-shelf neural network with sufficiently strong recurrence, a particularly interesting computational problem is short-term memory for associations between multiple objects or features, since the space of such associations is quite large. Since Hebbian plasticity is thought to be too slow for the fast timescales required by short-term memory, one well-known approach to this problem has focused on temporal synchrony between associated neurons, but such a mechanism is intrinsically fragile and requires a nontrivial encoder and decoder. An alternative model motivated by the expressive power of distributing representations across many neurons assumes that associations are held in persistent distributed states, but it still requires training of an encoder and decoder to store and retrieve the associations. Here we show how a network with completely random connections between item units and a small reservoir of bistable memory units (containing far fewer units than possible associations) can remember and recall multiple associations between items with excellent accuracy without any training or any separate encoder or decoder, and we demonstrate the model's biophysical plausibility by implementing it in a network of standard integrate-and-fire neurons. Fundamentally, the model relies on the compositionality of attractor states and on their ability to gate information transmission, and it predicts that short-term memory is limited primarily by the overlap of persistent brain states encoding different pieces of information. Importantly, the model illustrates how pre-existing, untrained connections to a reservoir of memory units could subserve a high-level cognitive task, thus lowering the architectural complexity required for this important brain function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "* associations are key part of short-term memory\n",
    "* feature binding\n",
    "* why synchrony hypothesis not sufficient\n",
    "* why hebbian association not sufficient\n",
    "* O'Reilly 2003 - alternatives to synchrony: distributed representations\n",
    "* O'Reilly 2006 - bistable neurons in PFC\n",
    "* distributed representations solves combinatorial explosion and are robust to damage\n",
    "* when dealing with memory must consider distributions not over neural activations but over \"stable\" attractor states\n",
    "* if you consider compositional attractor states you can get large number of them\n",
    "\n",
    "#### novel aspects\n",
    "\n",
    "* uses distributed representations over attractors, not neural activations -- thus extends to short-term memory\n",
    "* requires no explicit decoder (presuming bidirectional connectivity)\n",
    "* requires no training (e.g., adding new element requires only randomly connecting it to memory pool)\n",
    "\n",
    "#### computational properties\n",
    "\n",
    "* shows how random connections to \"reservoir\" of bistable units easily suffices for large amount of short-term associative memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
